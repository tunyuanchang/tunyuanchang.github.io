<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tun-Yuan Chang | Home</title>
    <link>https://tunyuanchang.github.io/</link>
    <description>Recent content on Tun-Yuan Chang | Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Oct 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://tunyuanchang.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>About Me</title>
      <link>https://tunyuanchang.github.io/page/intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://tunyuanchang.github.io/page/intro/</guid>
      <description>Welcome to Tun-Yuan&amp;rsquo;s personal website!</description>
    </item>
    <item>
      <title>[LAVA&#39;25] Harvesting Temporal Correlation in Large Vision-Language Models</title>
      <link>https://tunyuanchang.github.io/post/lava25/</link>
      <pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://tunyuanchang.github.io/post/lava25/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;Tun-Yuan Chang&lt;/strong&gt;, Kenneth Chandra, and Cheng-Hsin Hsu&lt;br&gt;&#xA;National Tsing Hua University, Hsinchu, Taiwan&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;div class=&#34;button-row&#34;&gt; &#xA;    &#xA;    &#xA;    &#xA;    &#xA;    &#xA;    &#xA;    &lt;a href=&#34;https://doi.org/10.1145/3728483.3760196&#34; class=&#34;btn btn-custom&#34;&gt;&#xA;        &lt;i class=&#34;fa fa-lg fa-file-pdf&#34;&gt;&lt;/i&gt; &#xA;         Paper&#xA;    &lt;/a&gt;&#xA;    &#xA;    &#xA;&#xA;&#xA;    &#xA;    &#xA;    &#xA;    &#xA;    &#xA;    &#xA;    &lt;a href=&#34;https://github.com/tunyuanchang/MM25_LAVA&#34; class=&#34;btn btn-custom&#34;&gt;&#xA;        &lt;i class=&#34;fab fa-lg fa-github&#34;&gt;&lt;/i&gt; &#xA;         Code&#xA;    &lt;/a&gt;&#xA;    &#xA;    &#xA;&#xA; &lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;figure fig-100&#34; &gt;&#xA;  &#xA;    &lt;img class=&#34;fig-img&#34; src=&#34;https://tunyuanchang.github.io/images/lava25_overview.jpg&#34; &gt;&#xA;  &#xA;  &#xA;&lt;/div&gt;&#xA;&#xA;&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;&#xA;&lt;p&gt;Temporal correlation across video frames captures important dynamics that may be used to enhance the effectiveness of various video understanding tasks.&#xA;To the best of our knowledge, we are the first to propose integrating Neural Network (NN)-based temporal embedders with ordinary Large Vision-Language Models (LVLMs) to leverage temporal correlation in diverse downstream tasks.&#xA;Unlike existing works in the literature that employ single-frame processing and frame concatenation, our proposed NN-based temporal embedders systematically encode temporal dependencies, thereby enriching the semantic understanding within LVLM frameworks. With diverse generator designs, the resulting LVLM-based NNs could work with diverse downstream video understanding tasks.&#xA;We validate the effectiveness and efficiency of our LVLM-based NNs using 3D human pose estimation as a representative case study.&#xA;Our extensive experiments with a popular dataset reveal that, compared to single-frame processing, our proposed NNs achieve much shorter training time, shorter inference time, and lower GPU memory consumption.&#xA;Among our proposed LVLM-based NNs, the CNN-based temporal embedder achieves the highest accuracy at a manageable overhead: with an MPJPE (Mean Per Joint Position Error) of 201.88 mm, per-frame inference time of 0.803 ms, and GPU memory consumption of only 45.83 MiB per frame.&#xA;With our experiment results, we answer the open research question:&#xA;&lt;strong&gt;Can temporal correlation across frames be harvested to improve video understanding with LVLMs, compared to conventional single-frame processing and frame concatenation?&lt;/strong&gt; In particular, our findings affirmatively answer this question, showing that explicitly capturing temporal correlation across frames enhances video understanding performance with LVLMs.&lt;/p&gt;&#xA;&lt;h3 id=&#34;design&#34;&gt;Design&lt;/h3&gt;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;figure fig-100&#34; &gt;&#xA;  &#xA;    &lt;img class=&#34;fig-img&#34; src=&#34;https://tunyuanchang.github.io/images/lava25_block.jpg&#34;  alt=&#34;Block diagram of our LVLM-based NNs for human pose estimation.&#34;&gt;&#xA;  &#xA;   &#xA;    &lt;span class=&#34;caption&#34;&gt;Block diagram of our LVLM-based NNs for human pose estimation.&lt;/span&gt;&#xA;  &#xA;&lt;/div&gt;&#xA;&#xA;&lt;h3 id=&#34;citation&#34;&gt;Citation&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-bibtex=&#34; data-lang=&#34;bibtex=&#34;&gt;@inproceedings{&#xA;    chang2025harvesting,&#xA;    title={Harvesting Temporal Correlation in Large Vision-Language Models: Using Pose Estimation as a Case Study},&#xA;    author={Chang, Tun-Yuan and Chandra, Kenneth and Hsu, Cheng-Hsin},&#xA;    booktitle={Proceedings of the 2nd International Workshop on Large Vision-Language Model Learning and Applications},&#xA;    pages={3--12},&#xA;    year={2025}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
  </channel>
</rss>
